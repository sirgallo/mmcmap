## Tests


## Tested On

`Macbook Pro 14 with M2Pro, 16 GB Mem, 500GB SSD`


## Concurrent Test

The concurrent test fills the `mmcmap` with randomly generated byte slices, where keys and values are the same and each byte slice is 32 randomly generated letters between a-z.
```
for each test, 3 total tests are taken for write/read/delete (for multi writer/reader, lock free, lock on dynamic resize, optimistic flushing)

100,000 key val pair:
  - concurrent write: 2.86, 2.66, 2.65 ~ 2.72 sec = 36,765 w/s
  - concurrent read: 0.37, 0.37, 0.34 ~ 0.36 sec = 277,778 r/s
  - concurrent delete: 2.12, 2.06, 2.40 sec ~ 2.19 = 45,662 w/s
  - file size in bytes: 262144000

1,000,000 key val pair:
  - concurrent write: 40.72, 38.85, 39.35 ~ 39.64 sec = 25,227 w/s
  - concurrent read: 4.11, 4.05, 3.95 ~ 4 sec = 250,000 r/s
  - concurrent delete: 27.60, 27.84, 27.12 ~ 27.52 sec = 36,337 w/s
  - file size in bytes: 2048576000

2,000,000 key val pair:
  - concurrent write: 81.29, 81.29, 82.86 ~ 81.81 sec = 24,447 w/s
  - concurrent read: 8.74, 8.42, 8.51 ~ 8.56 sec = 233,645 r/s
  - concurrent delete: 54.20, 54.21, 53.14 ~ 53.85 sec = 37,140 w/s
  - file size in bytes: 3048576000

4,000,000 key val pair:
  - concurrent write: 168.51, 159.20, 159.00 ~ 162.24 sec = 24,655 w/s
  - concurrent read: 16.43, 17.01, 16.99 ~ 16.81 sec = 237,954 r/s
  - concurrent delete: 108.28, 109.04, 110.19 ~ 109.17 sec = 36,640 w/s
  - file size in bytes: 7048576000


weights for each test are as follows:

total k/v pairs: 7,100,000
100,000: 0.014
1,000,000: 0.141
2,000,000: 0.282
4,000,000: 0.563

avg write = ((0.014 * (36,765 + 45,662)) + (0.141 * (25,227 + 36,337)) + (0.282 * (24,447 + 37,140)) + (0.563 * (24,655 + 36,640)))/2
avg read = (0.014 * 277,778) + (0.141 * 250,000) + (0.282 * 233,645) + (0.563 * 237,954)

avg write: 30,855.56 w/s
avg read: 238,994.88 r/s


single test with higher concurrency:

16,000,000 key val pair:
  - concurrent write: 600.93 sec = 26,667 w/s
  - concurrent read: 120.15 sec = 133,167 rs
  - concurrent delete: 407.64 sec = 39,250 w/s
  - file size in bytes: 27048576000
```


## Mixed Parallel Workload (Read/Write)

The parallel tests seeds a map with 1,000,000 randomly generated byte slices, where keys and values are the same and each byte slice is 32 randomly generated letters between a-z. 

The test involves a mixed workload of reads and writes, with 80% reads and 20% writes, where reads read the existing key-val pairs and writes create 200,000 new entries.
```

1,000,000 key val pair:
  - read existing: 12.66, 11.34, 11.56 ~ 11.85 sec = 67,510 r/s
  - write new: 10.65, 9.58, 9.27 ~ 9.83 sec = 20,346 w/s

2,000,000 key val pair:
  - read existing: 26.56, 25.00, 23.89 ~ 25.15 sec = 63,618 r/s
  - write new: 22.12, 21.55, 19.93 ~ 21.2 sec = 18,868 w/s

4,000,000 key val pair:
  - read existing: 55.80, 50.53, 58.23 ~ 54.85 sec = 58,341 r/s
  - write new: 46.90, 43.13, 50.56 ~ 46.86 sec = 17,072 w/s


weights for each test are as follows:

total k/v pairs: 7,000,000
1,000,000: 0.143
2,000,000: 0.286
4,000,000: 0.571

avg write =  (0.143 * 20,346) + (0.286 * 18,686) + (0.571 * 17,072)
avg read = (0.143 * 67,510) + (0.286 * 63,618) + (0.571 * 58,341)

avg write: 18,001.79 w/s
avg read: 61,161.39 r/s
```


## Afterthoughts

When mixed workload is introduced, there is a significant decrease in read performance and only a minor decrease in write performance. This can most likely be attributed to the scheduling of go routines and not favoring either reads or writes. Go routines are not true threads and are multiplexed onto a smaller number of system threads. When there is high contention for system resources, slower operations like writes may end up causing faster operations like reads to have to wait in a queue.

With higher concurrency and a larger memory mapped file, there may also be a higher likelihood of page faults when accessing data in the memory map that is not currently in memory but needs to be fetched by the OS from disk. This is not as much of an issue on systems with fast SSDs, but on HDD this can cause a significant bottleneck to the application and will cause higher latency for operations. A caching strategy or a different memory layout may be explored in later revisions.

With the current design being append only, this makes sure the data is immutable but because of this feature, the size of the memory mapped file will only ever increase and will contain outdated versions of data. In future revisions, a mechanism for compacting the memory mapped file will also be explored.